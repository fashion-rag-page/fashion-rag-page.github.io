<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Fashion-RAG: Multimodal Fashion Image Editing via Retrieval-Augmented Generation</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Fashion-RAG: Multimodal Fashion Image Editing via Retrieval-Augmented Generation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://furio1999.github.io/" target="_blank">Fulvio Sanguigni</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=UJ4D3rYAAAAJ&hl=en" target="_blank">Davide Morelli</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.it/citations?user=DzgmSJEAAAAJ&hl=it" target="_blank">Marcella Cornia</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.it/citations?user=OM3sZEoAAAAJ&hl=it" target="_blank">Rita Cucchiara</a><sup>1</sup>
              </span>
            </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>University of Modena and Reggio Emilia<br></span>
                    <span class="author-block"><sup>2</sup>University of Pisa<br></span>
                    <div>
                      International Joint Conference on Neural Networks (IJCNN) 2025<br>
                      <span style="color: red;">Oral Presentation</span>
                    </div>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2504.14011.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/furio1999/fashion-rag" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Project page link -->
                <span class="link-block">
                  <a href="https://huggingface.co/furio19/fashion-rag" target="_blank"
                     class="external-link button is-normal is-rounded is-warning">
                    <span class="icon">
                      <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" 
                           alt="Hugging Face" style="height: 1em; vertical-align: middle;">
                    </span>
                    <span>Hugging Face</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2504.14011" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/retrieve_demo.png" alt="Teaser Image" style="width: 50%; height: auto; display: block; margin: 0 auto;">
      <!--
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
      -->
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In recent years, the fashion industry has increasingly adopted AI technologies to enhance customer experience, 
            driven by the proliferation of e-commerce platforms and virtual applications. Among the various tasks, 
            virtual try-on and multimodal fashion image editing -- which utilizes diverse input modalities such as text, 
            garment sketches, and body poses -- have become a key area of research. Diffusion models have emerged 
            as a leading approach for such generative tasks, offering superior image quality and diversity. 
            However, most existing virtual try-on methods rely on having a specific garment input, 
            which is often impractical in real-world scenarios where users may only provide textual specifications. 
            To address this limitation, in this work we introduce Fashion Retrieval-Augmented Generation (Fashion-RAG), 
            a novel method that enables the customization of fashion items based on user preferences provided in textual form. 
            Our approach retrieves multiple garments that match the input specifications and generates a personalized image by 
            incorporating attributes from the retrieved items. To achieve this, we employ textual inversion techniques, where 
            retrieved garment images are projected into the textual embedding space of the Stable Diffusion text encoder, 
            allowing seamless integration of retrieved elements into the generative process. Experimental results on 
            the Dress Code dataset demonstrate that Fashion-RAG outperforms existing methods both qualitatively 
            and quantitatively, effectively capturing fine-grained visual details from retrieved garments. 
            To the best of our knowledge, this is the first work to introduce a retrieval-augmented generation approach 
            specifically tailored for multimodal fashion image editing.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Paper method -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title has-text-centered">Method</h2>

      <img src="static/images/model.png" style="width: 80%; height: auto; display: block; margin: 0 auto;" alt="Poster Image">
      <figcaption class="has-text-centered is-size-6 mt-2">
        Overview of the proposed retrieval-augmented multimodal fashion image editing framework. 
        The model leverages a diffusion-based inpainting pipeline, taking as input a masked reference image, 
        a pose map, a binary mask indicating the editable region, and multimodal conditioning signals, 
        including text descriptions and retrieved garments. Retrieved garments are projected into the CLIP textual 
        space and combined with the textual embeddings to enhance the U-Net cross-attention mechanism. 
        The U-Net iteratively denoises the latent representation over multiple steps, 
        and the VAE decoder generates the final image.
      </figcaption>
        
    </div>
  </div>
</section>
<!--End paper poster -->

<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title has-text-centered">Visual Comparisons</h2>

      <figure>
        <img src="static/images/Image2.png" style="width: 80%; height: auto; display: block; margin: 0 auto;" alt="Poster illustrating the Fashion-RAG model pipeline">
        <figcaption class="has-text-centered is-size-6 mt-2">
          Our method mitigates existing problems such as wrong color transfer and shape mismatch by using a retrieval-augmented generation approach.
          Previous method struggle to transfer some visual attributes to the model image vecause they are constrained to one single input 
          garment. Moreover, the garment item is provided by the user. In contrast, we relax the second assuption, requiring only text
          description of the garment to be edited. Our approach leverages the flexibility introduced by our RAG framework and 
          succesfully mixes up features from different garments.
        </figcaption>
      </figure>

    </div>
  </div>
</section>


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container has-text-centered">
      <figure>
        <img src="static/images/Fig3.png" style="width: 70%; height: auto; display: block; margin: 0 auto;" alt="Descriptive text for the first image" />
        <figcaption class="has-text-centered is-size-6 mt-2">
          We demonstrate the effectiveness of our method on the Dress Code dataset, showing that 
          Fashion-RAG improves with the number of retrieved images and can scale up to three retrieved garments.
        </figcaption>
      </figure>
    </div>
  </div>
</section>
<!-- End image carousel -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{sanguigni2025ijcnn,
        title={Fashion-RAG: Multimodal Fashion Image Editing via Retrieval-Augmented Generation},
        author={Sanguigni, Fulvio et Al.},
        booktitle={2025 International Joint Conference on Neural Networks (IJCNN)},
        year={2025},
        organization={IEEE},
      }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic-project-page-template Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
